version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  api:
    build: .
    command: uvicorn optimizer_service.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./optimizer_service:/app/optimizer_service
    ports:
      - "8000:8000"
    depends_on:
      - redis
    environment:
      - LLM_PROVIDER=llama_cpp
      - LLAMA_HOST=llama-cpp
      - LLAMA_PORT=8000
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=rpc://redis:6379/0
  worker:
    build: .
    command: celery -A optimizer_service.worker.celery_app worker --loglevel=info
    volumes:
      - ./optimizer_service:/app/optimizer_service
    depends_on:
      - redis
      - api
      - llama-cpp
    environment:
      - LLM_PROVIDER=llama_cpp
      - LLAMA_HOST=llama-cpp
      - LLAMA_PORT=8000
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=rpc://redis:6379/0
  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    command:
      - -m
      - /models/gemma-3-27b-it-q4_0.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - -c
      - "8192"
      - -ngl
      - "99"
    volumes:
      - ./models:/models:ro
    ports:
      - "8001:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/ | grep -q 'slots_idle' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
volumes:
  models: